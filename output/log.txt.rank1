[07/18 06:26:05] detectron2 INFO: Rank of current process: 1. World size: 4
[07/18 06:26:06] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 (default, Mar 20 2024, 19:58:24) [GCC 11.2.0]
numpy                            1.22.4
detectron2                       0.6 @/data_hdd/users/pengzelin/CAT-Seg_CVPR_to_8/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 11.8
detectron2 arch flags            /data_hdd/users/pengzelin/CAT-Seg_CVPR_to_8/detectron2/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.13.1+cu117 @/home/pengzelin/.conda/envs/pzl/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version                   550.67
CUDA_HOME                        /usr/local/cuda-11.8/bin
Pillow                           8.2.0
torchvision                      0.14.1+cu117 @/home/pengzelin/.conda/envs/pzl/lib/python3.8/site-packages/torchvision
torchvision arch flags           /home/pengzelin/.conda/envs/pzl/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.1
-------------------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[07/18 06:26:06] detectron2 INFO: Command line arguments: Namespace(config_file='configs/vitb_384_hyperbolic.yaml', dist_url='auto', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=['OUTPUT_DIR', 'output'], resume=True)
[07/18 06:26:06] detectron2 INFO: Contents of args.config_file=configs/vitb_384_hyperbolic.yaml:
_BASE_: config.yaml
MODEL:
  META_ARCHITECTURE: "CATSeg"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  SEM_SEG_HEAD:
    NAME: "CATSegHead"
    IN_FEATURES: ["res2", "res3", "res4"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 171
    TRAIN_CLASS_JSON: "datasets/coco.json"
    TEST_CLASS_JSON: "datasets/coco.json"
    CLIP_PRETRAINED: "ViT-B/16"
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEXT_GUIDANCE_DIM: 512
    TEXT_GUIDANCE_PROJ_DIM: 128
    APPEARANCE_GUIDANCE_DIM: 512
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    DECODER_DIMS: [64, 32]
    DECODER_GUIDANCE_DIMS: [256, 128]
    DECODER_GUIDANCE_PROJ_DIMS: [32, 16]
    NUM_LAYERS: 2
    NUM_HEADS: 4
    HIDDEN_DIMS: 128
    POOLING_SIZES: [2, 2]
    FEATURE_RESOLUTION: [24, 24]
    WINDOW_SIZES: 12
    ATTENTION_TYPE: "linear"
    CLIP_FINETUNE: "oft_both"
  PROMPT_ENSEMBLE_TYPE: "single"
INPUT:
  MIN_SIZE_TRAIN: (384, )
  MIN_SIZE_TRAIN_SAMPLING: "choice"
  MIN_SIZE_TEST: 640
  CROP:
    ENABLED: True
    TYPE: "absolute"
    SIZE: (384, 384)
  SIZE_DIVISIBILITY: 384 
  FORMAT: "RGB"
  DATASET_MAPPER_NAME: "mask_former_semantic"
SOLVER:
  IMS_PER_BATCH: 4 
  LR_SCHEDULER_NAME: WarmupCosineLR
  BASE_LR: 0.0002
  MAX_ITER: 80000
  BACKBONE_MULTIPLIER: 0.0
  CLIP_MULTIPLIER: 0.075
TEST:
  EVAL_PERIOD: 80000
  
[07/18 06:26:06] detectron2.utils.env INFO: Using a generated random seed 8894650
[07/18 06:28:13] detectron2 INFO: Rank of current process: 1. World size: 4
[07/18 06:28:14] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 (default, Mar 20 2024, 19:58:24) [GCC 11.2.0]
numpy                            1.22.4
detectron2                       0.6 @/data_hdd/users/pengzelin/CAT-Seg_CVPR_to_8/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 11.8
detectron2 arch flags            /data_hdd/users/pengzelin/CAT-Seg_CVPR_to_8/detectron2/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.13.1+cu117 @/home/pengzelin/.conda/envs/pzl/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version                   550.67
CUDA_HOME                        /usr/local/cuda-11.8/bin
Pillow                           8.2.0
torchvision                      0.14.1+cu117 @/home/pengzelin/.conda/envs/pzl/lib/python3.8/site-packages/torchvision
torchvision arch flags           /home/pengzelin/.conda/envs/pzl/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.1
-------------------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[07/18 06:28:14] detectron2 INFO: Command line arguments: Namespace(config_file='configs/vitb_384_hyperbolic.yaml', dist_url='auto', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=['OUTPUT_DIR', 'output'], resume=True)
[07/18 06:28:14] detectron2 INFO: Contents of args.config_file=configs/vitb_384_hyperbolic.yaml:
_BASE_: config.yaml
MODEL:
  META_ARCHITECTURE: "CATSeg"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  SEM_SEG_HEAD:
    NAME: "CATSegHead"
    IN_FEATURES: ["res2", "res3", "res4"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 171
    TRAIN_CLASS_JSON: "datasets/coco.json"
    TEST_CLASS_JSON: "datasets/coco.json"
    CLIP_PRETRAINED: "ViT-B/16"
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEXT_GUIDANCE_DIM: 512
    TEXT_GUIDANCE_PROJ_DIM: 128
    APPEARANCE_GUIDANCE_DIM: 512
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    DECODER_DIMS: [64, 32]
    DECODER_GUIDANCE_DIMS: [256, 128]
    DECODER_GUIDANCE_PROJ_DIMS: [32, 16]
    NUM_LAYERS: 2
    NUM_HEADS: 4
    HIDDEN_DIMS: 128
    POOLING_SIZES: [2, 2]
    FEATURE_RESOLUTION: [24, 24]
    WINDOW_SIZES: 12
    ATTENTION_TYPE: "linear"
    CLIP_FINETUNE: "oft_both"
  PROMPT_ENSEMBLE_TYPE: "single"
INPUT:
  MIN_SIZE_TRAIN: (384, )
  MIN_SIZE_TRAIN_SAMPLING: "choice"
  MIN_SIZE_TEST: 640
  CROP:
    ENABLED: True
    TYPE: "absolute"
    SIZE: (384, 384)
  SIZE_DIVISIBILITY: 384 
  FORMAT: "RGB"
  DATASET_MAPPER_NAME: "mask_former_semantic"
SOLVER:
  IMS_PER_BATCH: 4 
  LR_SCHEDULER_NAME: WarmupCosineLR
  BASE_LR: 0.0002
  MAX_ITER: 80000
  BACKBONE_MULTIPLIER: 0.0
  CLIP_MULTIPLIER: 0.075
TEST:
  EVAL_PERIOD: 80000
  
[07/18 06:28:14] detectron2.utils.env INFO: Using a generated random seed 17208288
[07/18 06:28:26] detectron2.engine.defaults INFO: Model:
CATSeg(
  (sem_seg_head): CATSegHead(
    (predictor): CATSegPredictor(
      (clip_model): CLIP(
        (visual): VisualTransformer(
          (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
          (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (transformer): Transformer(
            (resblocks): Sequential(
              (0): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (1): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (2): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (3): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (4): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (5): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (6): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (7): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (8): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (9): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (10): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (11): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (1): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (2): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (3): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (4): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (5): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (6): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (7): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (8): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (9): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (10): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (11): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (token_embedding): Embedding(49408, 512)
        (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Aggregator(
        (layers): ModuleList(
          (0): AggregatorLayer(
            (swin_block): SwinTransformerBlockWrapper(
              (block_1): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (block_2): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (guidance_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (attention): ClassTransformerLayer(
              (pool): AvgPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0)
              (attention): AttentionLayer(
                (q): Linear(in_features=256, out_features=128, bias=True)
                (k): Linear(in_features=256, out_features=128, bias=True)
                (v): Linear(in_features=128, out_features=128, bias=True)
                (attention): LinearAttention()
              )
              (MLP): Sequential(
                (0): Linear(in_features=128, out_features=512, bias=True)
                (1): ReLU()
                (2): Linear(in_features=512, out_features=128, bias=True)
              )
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): AggregatorLayer(
            (swin_block): SwinTransformerBlockWrapper(
              (block_1): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (block_2): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (guidance_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (attention): ClassTransformerLayer(
              (pool): AvgPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0)
              (attention): AttentionLayer(
                (q): Linear(in_features=256, out_features=128, bias=True)
                (k): Linear(in_features=256, out_features=128, bias=True)
                (v): Linear(in_features=128, out_features=128, bias=True)
                (attention): LinearAttention()
              )
              (MLP): Sequential(
                (0): Linear(in_features=128, out_features=512, bias=True)
                (1): ReLU()
                (2): Linear(in_features=512, out_features=128, bias=True)
              )
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
        (guidance_projection): Sequential(
          (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU()
        )
        (text_guidance_projection): Sequential(
          (0): Linear(in_features=512, out_features=128, bias=True)
          (1): ReLU()
        )
        (decoder_guidance_projection): ModuleList(
          (0): Sequential(
            (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
          (1): Sequential(
            (0): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
        )
        (decoder1): Up(
          (up): ConvTranspose2d(128, 96, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(4, 64, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(4, 64, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (decoder2): Up(
          (up): ConvTranspose2d(64, 48, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(2, 32, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(2, 32, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (head): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (upsample1): ConvTranspose2d(768, 256, kernel_size=(2, 2), stride=(2, 2))
  (upsample2): ConvTranspose2d(768, 128, kernel_size=(4, 4), stride=(4, 4))
)
[07/18 06:28:29] detectron2.data.datasets.coco INFO: Loaded 118287 images with semantic segmentation from ../OV_seg/coco-stuff/images/train2017
[07/18 06:28:29] detectron2.data.build INFO: Using training sampler TrainingSampler
[07/18 06:28:31] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[07/18 06:28:31] detectron2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[07/18 06:28:31] detectron2.data.common INFO: Serialized dataset takes 20.08 MiB
[07/18 06:28:31] detectron2.data.build INFO: Making batched data loader with batch_size=1
[07/18 06:28:31] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from  ...
[07/18 06:28:31] fvcore.common.checkpoint INFO: No checkpoint found. Initializing model from scratch
[07/18 06:28:31] detectron2.engine.train_loop INFO: Starting training from iteration 0
[07/18 06:28:50] detectron2.engine.hooks INFO: Total training time: 0:00:18 (0:00:00 on hooks)
[07/18 06:31:31] detectron2 INFO: Rank of current process: 1. World size: 4
[07/18 06:31:32] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 (default, Mar 20 2024, 19:58:24) [GCC 11.2.0]
numpy                            1.22.4
detectron2                       0.6 @/data_hdd/users/pengzelin/CAT-Seg_CVPR_to_8/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 11.8
detectron2 arch flags            /data_hdd/users/pengzelin/CAT-Seg_CVPR_to_8/detectron2/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.13.1+cu117 @/home/pengzelin/.conda/envs/pzl/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1,2,3                      NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version                   550.67
CUDA_HOME                        /usr/local/cuda-11.8/bin
Pillow                           8.2.0
torchvision                      0.14.1+cu117 @/home/pengzelin/.conda/envs/pzl/lib/python3.8/site-packages/torchvision
torchvision arch flags           /home/pengzelin/.conda/envs/pzl/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.5.1
-------------------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[07/18 06:31:32] detectron2 INFO: Command line arguments: Namespace(config_file='configs/vitb_384_hyperbolic.yaml', dist_url='auto', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=['OUTPUT_DIR', 'output'], resume=True)
[07/18 06:31:32] detectron2 INFO: Contents of args.config_file=configs/vitb_384_hyperbolic.yaml:
_BASE_: config.yaml
MODEL:
  META_ARCHITECTURE: "CATSeg"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  SEM_SEG_HEAD:
    NAME: "CATSegHead"
    IN_FEATURES: ["res2", "res3", "res4"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 171
    TRAIN_CLASS_JSON: "datasets/coco.json"
    TEST_CLASS_JSON: "datasets/coco.json"
    CLIP_PRETRAINED: "ViT-B/16"
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEXT_GUIDANCE_DIM: 512
    TEXT_GUIDANCE_PROJ_DIM: 128
    APPEARANCE_GUIDANCE_DIM: 512
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    DECODER_DIMS: [64, 32]
    DECODER_GUIDANCE_DIMS: [256, 128]
    DECODER_GUIDANCE_PROJ_DIMS: [32, 16]
    NUM_LAYERS: 2
    NUM_HEADS: 4
    HIDDEN_DIMS: 128
    POOLING_SIZES: [2, 2]
    FEATURE_RESOLUTION: [24, 24]
    WINDOW_SIZES: 12
    ATTENTION_TYPE: "linear"
    CLIP_FINETUNE: "hyperbolic"
  PROMPT_ENSEMBLE_TYPE: "single"
INPUT:
  MIN_SIZE_TRAIN: (384, )
  MIN_SIZE_TRAIN_SAMPLING: "choice"
  MIN_SIZE_TEST: 640
  CROP:
    ENABLED: True
    TYPE: "absolute"
    SIZE: (384, 384)
  SIZE_DIVISIBILITY: 384 
  FORMAT: "RGB"
  DATASET_MAPPER_NAME: "mask_former_semantic"
SOLVER:
  IMS_PER_BATCH: 4 
  LR_SCHEDULER_NAME: WarmupCosineLR
  BASE_LR: 0.0002
  MAX_ITER: 80000
  BACKBONE_MULTIPLIER: 0.0
  CLIP_MULTIPLIER: 0.075
TEST:
  EVAL_PERIOD: 80000
  
[07/18 06:31:32] detectron2.utils.env INFO: Using a generated random seed 34777087
[07/18 06:31:44] detectron2.engine.defaults INFO: Model:
CATSeg(
  (sem_seg_head): CATSegHead(
    (predictor): CATSegPredictor(
      (clip_model): CLIP(
        (visual): VisualTransformer(
          (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
          (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (transformer): Transformer(
            (resblocks): Sequential(
              (0): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (1): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (2): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (3): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (4): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (5): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (6): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (7): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (8): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (9): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (10): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
              (11): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
                )
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (hyperbolic_attn): Adapter_init(
                  (adapter_attn_q): BlockDiagonalLinear()
                  (adapter_attn_k): BlockDiagonalLinear()
                  (adapter_attn_v): BlockDiagonalLinear()
                )
                (dp): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (1): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (2): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (3): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (4): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (5): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (6): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (7): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (8): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (9): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (10): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
            (11): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=2048, out_features=512, bias=True)
              )
              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (hyperbolic_attn): Adapter_init_text(
                (adapter_attn_q): BlockDiagonalLinear_text()
                (adapter_attn_k): BlockDiagonalLinear_text()
                (adapter_attn_v): BlockDiagonalLinear_text()
              )
              (dp): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (token_embedding): Embedding(49408, 512)
        (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Aggregator(
        (layers): ModuleList(
          (0): AggregatorLayer(
            (swin_block): SwinTransformerBlockWrapper(
              (block_1): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (block_2): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (guidance_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (attention): ClassTransformerLayer(
              (pool): AvgPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0)
              (attention): AttentionLayer(
                (q): Linear(in_features=256, out_features=128, bias=True)
                (k): Linear(in_features=256, out_features=128, bias=True)
                (v): Linear(in_features=128, out_features=128, bias=True)
                (attention): LinearAttention()
              )
              (MLP): Sequential(
                (0): Linear(in_features=128, out_features=512, bias=True)
                (1): ReLU()
                (2): Linear(in_features=512, out_features=128, bias=True)
              )
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): AggregatorLayer(
            (swin_block): SwinTransformerBlockWrapper(
              (block_1): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (block_2): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (guidance_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (attention): ClassTransformerLayer(
              (pool): AvgPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0)
              (attention): AttentionLayer(
                (q): Linear(in_features=256, out_features=128, bias=True)
                (k): Linear(in_features=256, out_features=128, bias=True)
                (v): Linear(in_features=128, out_features=128, bias=True)
                (attention): LinearAttention()
              )
              (MLP): Sequential(
                (0): Linear(in_features=128, out_features=512, bias=True)
                (1): ReLU()
                (2): Linear(in_features=512, out_features=128, bias=True)
              )
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
        (guidance_projection): Sequential(
          (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU()
        )
        (text_guidance_projection): Sequential(
          (0): Linear(in_features=512, out_features=128, bias=True)
          (1): ReLU()
        )
        (decoder_guidance_projection): ModuleList(
          (0): Sequential(
            (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
          (1): Sequential(
            (0): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
        )
        (decoder1): Up(
          (up): ConvTranspose2d(128, 96, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(4, 64, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(4, 64, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (decoder2): Up(
          (up): ConvTranspose2d(64, 48, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(2, 32, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(2, 32, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (head): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (upsample1): ConvTranspose2d(768, 256, kernel_size=(2, 2), stride=(2, 2))
  (upsample2): ConvTranspose2d(768, 128, kernel_size=(4, 4), stride=(4, 4))
)
[07/18 06:31:47] detectron2.data.datasets.coco INFO: Loaded 118287 images with semantic segmentation from ../OV_seg/coco-stuff/images/train2017
[07/18 06:31:47] detectron2.data.build INFO: Using training sampler TrainingSampler
[07/18 06:31:47] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[07/18 06:31:47] detectron2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[07/18 06:31:47] detectron2.data.common INFO: Serialized dataset takes 20.08 MiB
[07/18 06:31:47] detectron2.data.build INFO: Making batched data loader with batch_size=1
[07/18 06:31:48] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from  ...
[07/18 06:31:48] fvcore.common.checkpoint INFO: No checkpoint found. Initializing model from scratch
[07/18 06:31:48] detectron2.engine.train_loop INFO: Starting training from iteration 0
[07/18 06:32:40] detectron2.engine.hooks INFO: Overall training speed: 29 iterations in 0:00:17 (0.5978 s / it)
[07/18 06:32:40] detectron2.engine.hooks INFO: Total training time: 0:00:17 (0:00:00 on hooks)
